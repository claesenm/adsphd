\section{Introduction} \label{introduction}

Model evaluation is a critical step in the learning process. Typically, evaluations either report summary metrics, such as accuracy, F1 score, or area under the receiver operator characteristic (ROC) curve or visually show a model's performance under different operating conditions by using ROC or precision-recall curves. All the aforementioned evaluation approaches require constructing contingency tables (also called confusion matrices), which show how a model's predicted labels relate to an example's ground truth label. Computing a contingency table requires labeled examples. However, for many problems only a few labeled examples and many unlabeled ones are available as acquiring labels can be time-consuming, costly, unreliable, and in some cases impossible.

The field semi-supervised learning \citep{chapelle2006semi} focuses on coping with partially labeled data. Positive and unlabeled (PU) learning is a special case of semi-supervised learning where each example's label is either positive or not known~\citep{Liu:2003:BTC:951949.952139, yu2004pebl, denis2005learning, Elkan:2008:LCO:1401890.1401920, scottblanchard,mordelet2014bagging, Claesen2015resvm}. Both semi-supervised and PU learning tend to focus on developing learning algorithms that cope with partially labeled data during training as opposed to evaluating algorithms when the test set is partially labeled. What is less well studied is the effect of partially labeled data on evaluation. Currently, algorithms are evaluated assuming that the test data is fully labeled \citep{goldman2000enhancing, nigam2000text, chawla2005learning, calvo2007learning, chapelle2008optimization, mordelet2014bagging, Claesen2015resvm} and if the test data is only partially labeled, sometimes it is assumed that all unlabeled instances are negative when evaluating performance~\citep{mordelet2011prodige, sifrim2013extasy,sechidis2014statistical}.

This paper describes how to incorporate the unlabeled data in the model evaluation process. We show how to compute contingency tables based on only positive and unlabeled examples where the unlabeled set contains both positive and negative examples, by looking at the ranking of examples produced by a model.
Theoretically, we establish important relationships between contingency tables and rank distributions, which allow us to provide bounds on the false positive rate at each rank when the ranking contains examples whose ground truth label is unknown. Our findings have important implications for model selection as we show that naively assuming that all unlabeled examples are negative, as is sometimes done in PU learning, could lead to selecting the wrong model. We demonstrate the efficacy of our approach by estimating ROC and PR curves from real-world data.
