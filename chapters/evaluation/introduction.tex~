\section{Introduction} \label{introduction}
%Evaluating the quality of a predictive model is a critical step in the learning process. Typically, evaluations either report summary metrics, such as accuracy, F1 score, or area under the receiver operator characteristic (ROC) curve or visually show a model's performance under different operating conditions by using ROC or precision-recall curves. All the aforementioned evaluation approaches require constructing contingency tables (also called confusion matrices), which show how a model's predicted labels relate to an example's ground truth label. Computing a contingency table requires labeled examples. However, for many problems only a few labeled examples and many unlabeled ones are available~\citep{yarowsky1995unsupervised, weston2005semi} as acquiring labels can be time-consuming, costly, unreliable, and in some cases impossible.

%The field semi-supervised learning \citep{blum1998combining, chapelle2006semi, raina2007self} focuses on coping with partially labeled data.  Positive and unlabeled (PU) learning is a special case of semi-supervised learning where each example's label is either positive or not known~\citep{liu2003building, yu2004pebl, denis2005learning, Elkan:2008:LCO:1401890.1401920, mordelet2014bagging, claesen2014robust}. Both semi-supervised and PU learning tend to focus on developing learning algorithms that cope with partial labeled data during training as opposed to evaluating algorithms when the test set is partially labeled. What is less well studied is the effect of partially labeled data on evaluation. Currently, algorithms are evaluated assuming that the test data is fully labeled \citep{goldman2000enhancing, nigam2000text, chawla2005learning, calvo2007learning, chapelle2008optimization, mordelet2014bagging, claesen2014robust} and if the test data is only partially labeled, sometimes it is assumed that all unlabeled instances are negative~\citep{mordelet2011prodige, sifrim2013extasy}.

%Evaluating the quality of a predictive model is a critical step in the learning process. In many applications (e.g., medicine), a model must be rigorously evaluated before it can be deployed~\citep{pencina2008evaluating}.  Typically, evaluations often either report summary metrics, such as accuracy, F1 score, or area under the receiver operator characteristic (ROC) curve or visually show a model's performance under different operating conditions by using ROC or precision-recall curves. All these evaluation approaches require constructing contingency tables (also called confusion matrices), which show how a model's predicted labels relate to the ground truth labels, by depicting counts of true positives, false positives, true negatives and false negatives, denoted by $\TP$, $\FP$, $\TN$ and $\FN$, respectively. case}.

%However, computing a contingency table requires labeled examples. Yet, for many problems only a few labeled examples and many unlabeled ones are available as acquiring labels can be time-consuming, costly, unreliable, and in some cases impossible. This setting is known as semi-supervised learning, \citep{chapelle2006semi}.  PU learning is a special case of semi-supervised learning in which only positive and unlabeled data is given \citep{liu2003building, yu2004pebl, Elkan:2008:LCO:1401890.1401920, mordelet2014bagging, claesen2014robust, sechidis2014statistical}. 

%The majority of the research in semi-supervised learning and PU learning focuses on developing learning algorithms that cope with partial labeling during training as opposed to evaluating algorithms when the test set is partially labeled. Typically, the empirical evaluations assume that all test labels are known \citep{mordelet2014bagging, claesen2014robust, chawla2005learning, calvo2007learning}. While such an evaluation can corroborate the superiority of a new learning method, it does not provide a means for assessing the quality of a model if the test set is only partially labeled. 
%Alternatively, in the PU setting it is sometimes assumed that all unlabeled instances are negative while evaluating learned models~\citep{mordelet2011prodige, sechidis2014statistical}. However, we will show that this assumption could lead to a poor estimate of a model's performance in certain circumstances. 

Model evaluation is a critical step in the learning process. Typically, evaluations either report summary metrics, such as accuracy, F1 score, or area under the receiver operator characteristic (ROC) curve or visually show a model's performance under different operating conditions by using ROC or precision-recall curves. All the aforementioned evaluation approaches require constructing contingency tables (also called confusion matrices), which show how a model's predicted labels relate to an example's ground truth label. Computing a contingency table requires labeled examples. However, for many problems only a few labeled examples and many unlabeled ones are available as acquiring labels can be time-consuming, costly, unreliable, and in some cases impossible.

%However, for many problems only a few labeled examples and many unlabeled ones are available~\citep{yarowsky1995unsupervised, weston2005semi} as acquiring labels can be time-consuming, costly, unreliable, and in some cases impossible.

The field semi-supervised learning \citep{chapelle2006semi} focuses on coping with partially labeled data. Positive and unlabeled (PU) learning is a special case of semi-supervised learning where each example's label is either positive or not known~\citep{liu2003building, yu2004pebl, denis2005learning, Elkan:2008:LCO:1401890.1401920, scottblanchard,mordelet2014bagging, claesen2014robust}. Both semi-supervised and PU learning tend to focus on developing learning algorithms that cope with partially labeled data during training as opposed to evaluating algorithms when the test set is partially labeled. What is less well studied is the effect of partially labeled data on evaluation. Currently, algorithms are evaluated assuming that the test data is fully labeled \citep{goldman2000enhancing, nigam2000text, chawla2005learning, calvo2007learning, chapelle2008optimization, mordelet2014bagging, claesen2014robust} and if the test data is only partially labeled, sometimes it is assumed that all unlabeled instances are negative when evaluating performance~\citep{mordelet2011prodige, sifrim2013extasy,sechidis2014statistical}.

This paper describes how to incorporate the unlabeled data in the model evaluation process. We show how to compute contingency tables based on only positive and unlabeled examples where the unlabeled set contains both positive and negative examples, by looking at the ranking of examples produced by a model.
Theoretically, we establish important relationships between contingency tables and rank distributions, which allow us to provide bounds on the false positive rate at each rank when the ranking contains examples whose ground truth label is unknown. Our findings have important implications for model selection as we show that naively assuming that all unlabeled examples are negative, as is sometimes done in PU learning, could lead to selecting the wrong model. We demonstrate the efficacy of our approach by estimating ROC and PR curves from real-world data.

%and demonstrate the efficacy of our approach by estimating ROC and PR curves from real-world data. Our findings have implications for model selection and, in particular, show that naively assuming that all unlabeled examples are negative could lead to selecting the wrong model.

%This paper describes how to incorporate the unlabeled data in the model evaluation process. We show how to compute contigency tables based on only positive and unlabeled examples, where the unlabeled set contains both positive and negative examples. Our approach is based on looking at the ranking of examples produced by a model. Concretely, we establish important relationships between contingency tables and rank distributions, which allow us to compute contingency tables based only on the positions of known positives in an overall ranking. Theoretically, we give a lower and upper bound on the false positive rate at each rank. While our analysis focuses on the PU setting, we indicate where the analysis and procedure can incorpate negative examples. Empirically, we demonstrate the efficacy of our approach by estimating ROC and PR curves from real-world data. 

%We emphasize this setting because the absence of negative labels makes model evaluation extra challenging. A solution for PU learning could be enhanced to use negative labels when available.

%In the next Sections, we introduce the key concepts of our approach, describe the partial labeling context in detail and summarize the construction, properties and usage of ROC curves. Subsequently, we prove important relationships between contingency tables and rank distributions in Section~\ref{rank-roc}, which allow us to compute contingency tables based only on the positions of known positives in an overall ranking. These relationships are used in Section~\ref{practical} to formalize an efficient, practical approach to estimate contingency tables corresponding to a lower and upper bound on false positive rate at each rank. Finally, in Section~\ref{results} we show empirical results by estimating ROC and PR curves.


