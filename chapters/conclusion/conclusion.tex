\chapter{Conclusion}\label{ch:conclusion}

In this Section we will summarize the main insights and implications of the project, along with interesting avenues for future research. We will discuss machine learning-specific aspects in Section~\ref{conclusion:ml} and the screening application in Section~\ref{conclusion:screening}.

%An extensive conclusion, including a global discussion of the research results, a discussion of the implications of the PhD research and future perspectives in regards to follow-up research.

\section{Machine learning contributions} \label{conclusion:ml}
The machine learning research in this project focused on learning from positive and unlabeled data and the construction of high-quality, reusable tools that allow easy reproduction of our results, fast prototyping of novel ideas and the partial automation of machine learning pipelines.

One of the main hurdles we had to overcome was learning classifiers from positive and unlabeled data, where the set of labeled positives is known to be contaminated with some false positives (Chapter~\ref{ch:resvm}). Existing approaches were sensitive to false positives, often to such an extent that they became unreliable. Our approach addresses this weakness by resampling known positives within a bagging framework, which was a natural extension to the already-existing bagging SVM that used the same idea on the unlabeled set \citep{mordelet2014bagging}.

The major issue we tackled in semi-supervised learning was evaluating binary classifiers without known negatives (Chapter~\ref{ch:evaluation}). Prior to our work, a lack of negatives prohibited the quantification of classifier performance in terms of traditional metrics, which in turn prohibited the use of (potentially very good) models for many applications (e.g., the performance of models used for screening or diagnosis must be quantified before their use is even considered). Additionally, we have shown that existing model selection approaches are prone to errors in some practical cases. 

The software packages we developed (described in Chapters~\ref{ch:ensemblesvm} and~\ref{ch:optunity}) provide easy access to our methods to other researchers and enable them to reuse and extend our work, rather than having to reinvent the wheel. With these packages we gave heed to the call of several prominent journals regarding the need and value of open-source software in scientific research \citep{sonnenburg2007need,prlic2012ten}. Optunity specifically tackles a common element of practical machine learning (Chapter~\ref{ch:optunity}), as most methods feature hyperparameters that must be optimized somehow. Optunity's usefulness is evidenced by its popularity, with hundreds of monthly downloads through the Python Package Index at the time of writing.\footnote{Statistics can be found at \url{https://pypi.python.org/pypi/Optunity}.}

\subsection{Future work} \label{conclusion:ml-future}
Based on the experience gathered and results obtained during our work, we see opportunities for future machine learning research in automated hyperparameter optimization and learning from positive and unlabeled data.

Fully automated machine learning is becoming popular, evidenced by competitions like the ChaLearn AutoML challenge \citep{guyondesign}. A key element of such pipelines is automated hyperparameter optimization, which is receiving a lot of research attention \citep{bergstra2011algorithms, snoek2012practical, bergstra2012random, bergstra2013making, eggensperger2013towards, martinez2014bayesopt, claesen2014easy, eggensperger2015efficient}. Current research focuses on Bayesian optimization based on the somewhat dogmatic belief that these optimizers converge faster than others. We are skeptical towards this claim, as meta-heuristic techniques have been shown competitive in recent publications \citep{papa2015model} as well as our own experiments (see Chapter~\ref{ch:optunity}) and because the famous no free lunch theorem  applies \citep{wolpert1997no}. We believe that meta-heuristic techniques for hyperparameter optimization are currently underdeveloped but promising.

In Chapter~\ref{ch:evaluation} we described how to compute any contingency table-based metric based on only positive and unlabeled data. This is the first approach to compute commonly known metrics, and hence an important contribution, though it does not directly enable computing probablistic performance metrics in general and strictly proper scoring rules like log loss or Brier score in particular. Strictly proper scoring rules \citep{gneiting2007strictly} are especially useful for model selection and calibration as they are more sensitive than metrics like AUROC and AUPR, so extending our approach to enable approximation of these metrics would be useful.


\section{Screening for type 2 diabetes} \label{conclusion:screening}
We set out to investigate the extent to which health expenditure data enables clinical applications like screening for T2D, and thereby improve healthcare. During this research we have successfully developed a proof-of-concept screening method for type 2 diabetes, based exclusively on readily-available health expenditure data collected by the largest Belgian mutual health insurer. Our performance benchmarks have indicated competitiveness to existing screening approaches for T2D that have proven useful in international contexts. Our work can serve as the basis for cost-effective population-wide screening for type 2 diabetes which would strengthen secondary prevention of the disease.

Principally, our screening method identifies patients that likely require glucose lowering agents. This omits patients with less progressed diabetes. Given the way we approached the task, we likely have problems identifying patients suffering from mild (pre)diabetes. That said, we treated such patients as negatives in all performance evaluations, and hence the performance we reported is correctly estimated. In fact, our performance estimates are conservative under the reasonable assumption that the claims history of patients with mild diabetes is somewhere between the claims histories of healthy patients and those of patients with GLA-treated diabetes. Finally, additional experiments indicate that our approach can identify patients years before GLA therapy is initiated.

\subsection{Future work} 

Our research has shown that health expenditure data can effectively be used as a basis for T2D screening programmes with state-of-the-art performance, despite the fact it lacks information on known risk factors for diabetes which are heavily used by other screening approaches (e.g., lifestyle, BMI, genetic predisposition, \ldots). This suggests a lot of potential in screening methods that combine both of these types of information, that is health expenditure data \emph{and} lifestyle, BMI and various clinical parameters, though the potential improvement in predictive performance by linking data sources is difficult to estimate a priori. 

Health expenditure data unites information across caregivers and provides a fairly complete long-term overview, while other data sources include crucial parameters about lifestyle, genetics and clinical measurements. As such, it is reasonable to assume that these types of data are complementary, and hence their union may allow for screening approaches that far outperform existing approaches. A lot of this information could simply be obtained via patient questionnaires (e.g. BMI, lifestyle, family history, \ldots), though more detailed clinical parameters are harder to procure. Overall, this is a very promising direction for future research, though coupling health expenditure data with other types of information is a sensitive subject from a privacy point of view, so strict adherence to all guidelines and regulations is of paramount importance. 

\subsection{Health expenditure data}
The screening method itself is a proof-of-concept which showcases the potential clinical value of administrative databases such as claims databases maintained by mutual health insurers. The results of the project yield a few conclusions regarding the use of health expenditure data for clinical applications:

\begin{itemize}
\item It is a valuable source of information to build screening programmes for diseases like type 2 diabetes, which prove challenging to detect in contemporary medicine. Its key strengths are that it integrates healthcare information across all caregivers and provides a reliable longitudinal overview of a patient's medical resource-use history. However, resource-use histories are not as detailed as clinical databases maintained by caregivers.
\item It is difficult to find or replicate these strengths elsewhere. Particularly, Belgium does not yet have EHRs that record information from all medical stakeholders into a central, comprehensive database. Implementing such EHRs would be complex for technological, legal, political and psychological reasons. Initiatives like Vitalink\footnote{More info about Vitalink is available at http://www.vitalink.be/.} and the shared pharmaceutical file\footnote{In Dutch: gedeeld farmaceutisch dossier (GFD).} envision parts of this functionality, though these projects are currently in their infancy. For now, claims records remain the sole source of complete, long-term information across medical stakeholders and time.
\end{itemize}

Health expenditure data is already widely used for epidemiology \citep{pladevall2004clinical,lee2006medication,garg2010acute,s23} and, more recently, to assess quality of care \citep{kcequality}. Our work shows that claims data can additionally be used pro-actively to improve healthcare, rather than only in retrospective, descriptive studies. The wealth of information in these databases can likely improve healthcare in many aspects.


\subsection{The elephant in the room}
Our work demonstrated the technological possibility of screening for T2D based on health expenditure data. However, we have not touched upon the ethical, legal and psychological perspectives of data-driven applications in the health and healthcare domain. We conclude this work by briefly discussing some key barriers and open questions concerning applications such as ours.

\subsubsection{Population-wide screening}

The approach outlined in this thesis enables population-wide screening for T2D at a very low operational cost, since we exclusively use readily-available health expenditure data. A major practical question for any population-wide screening approach is how it should be put to use in the real world. 

The first option is to perform risk assessment on the whole population (all members of NACM in our context) and actively contact persons identified to be at high risk. From a medical perspective, such a \emph{push} model seems most appropriate since this has the potential to help a maximal amount of people. 

On the other hand, many people may have qualms about inferences of their health status without the concerned person's explicit prior request and/or approval. A push model as described above can raise issues from a psychological point of view, as it can induce a ``\emph{Big Brother}'' feeling that will surely be resisted by some. Additionally, the \emph{right not to know} has experienced a revived interest from an ethical point of view due to advances in genetic research \citep{ost1984right, andorno2004right, chadwick2014right}, and many of the arguments raised in that discussion essentially also apply to push models which involve proactively notifying patients identified at high risk for some disease.\footnote{The right not to know entails whether patients must be informed regarding some (potentially serious) health problems, even if the patients themselves do not want to know.} A \emph{pull} model, which allows everyone to request the results of a personalized risk analysis at their own leisure, may prove to be more popular. Note that, even in a pull model, the risk assessment can potentially be carried out in advance, just without notifying the associated patients of their outcome.

\subsubsection{Use of personal data}

Principally, health and healthcare data is considered personal information owned by the involved patient, though often this patient has limited or no access to his or her data and may even be unaware of its existence. This highly sensitive type of personal data is rightfully protected from unpermitted use \citep{privacywet,eu_data}.

Several models exist for patients to permit the usage of personal data for research and various applications after obtaining informed consent. The main distinction is between \emph{opt-in} models, in which data usage is permitted after explicit approval of the patient, and \emph{opt-out} models, in which data can be used unless patients explicitly prohibit it. Opt-in models are most common due to their conservative nature from a privacy point of view, but these may prohibit applications that require a lot of data to create a minimum viable product. 

The use of health and healthcare data is a sensitive matter and policymakers in Belgium and Europe understandably err on the side of conservatism \citep{privacywet, eu_data}, though this inevitably constrains both research and potentially beneficial applications. Recently, the proposal for European data protection regulation raised a lot of criticism, as researchers in health and healthcare worried that an increased amount of conservatism could significantly impede scientific research \citep{feam_2, cumbley2013big, fears2014data, wellcome_position_statement, feam_1}. Our project chimes in on this discussion with a proof-of-concept clinical application based on health expenditure data.

In the end, policymakers must strike a tradeoff between patient privacy vis-\`a-vis potential healthcare improvements through the use of personal data. This sensitive matter is the subject of ongoing debate in both Belgium and Europe. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Keep the following \cleardoublepage at the end of this file, 
% otherwise \includeonly includes empty pages.
\cleardoublepage

% vim: tw=70 nocindent expandtab foldmethod=marker foldmarker={{{}{,}{}}}
