\chapter{Conclusion}\label{ch:conclusion}

We conclude by summarizing the main insights and implications of the project, along with interesting avenues for future research. We will discuss machine learning-specific aspects in Section~\ref{conclusion:ml} and the screening application in Section~\ref{conclusion:screening}.

%An extensive conclusion, including a global discussion of the research results, a discussion of the implications of the PhD research and future perspectives in regards to follow-up research.

\section{Machine learning contributions} \label{conclusion:ml}
The machine learning research in this project focused on learning from positive and unlabeled data and the construction of high-quality, reusable tools that allow easy reproduction of our results, fast prototyping of novel ideas and the partial automation of machine learning pipelines.

One of the main hurdles we had to overcome was learning classifiers from positive and unlabeled data, where the set of labeled positives is known to be contaminated with some false positives (Chapter~\ref{ch:resvm}). Existing approaches were sensitive to false positives, often to such an extent that they became unreliable. Our approach addresses this weakness by resampling known positives within a bagging framework, which was a natural extension to the already-existing bagging SVM that used the same idea on the unlabeled set \citep{mordelet2014bagging}.

The major issue we tackled in semi-supervised learning was evaluating binary classifiers without known negatives (Chapter~\ref{ch:evaluation}). Prior to our work, a lack of negatives prohibited the quantification of classifier performance in terms of traditional metrics, which in turn prohibited the use of (potentially very good) models for many applications (e.g., the performance of models used for screening or diagnosis must be quantified before their use is even considered). Our contribution solves an important problem that plagues many practical applications. Specifically, the ability to evaluate models without known negatives saved us a considerable amount of time, as the only alternative would have been to acquire negative labels manually. Additionally, we have shown that existing model selection approaches are prone to errors in some practical cases. 

The software packages we developed (described in Chapters~\ref{ch:ensemblesvm} and~\ref{ch:optunity}) provide easy access to our methods to other researchers and enable them to reuse and extend our work, rather than having to reinvent the wheel. With these packages we gave heed to the call of several prominent journals regarding the need and value of open-source software in scientific research \citep{sonnenburg2007need,prlic2012ten}. Optunity specifically tackles a common element of practical machine learning (Chapter~\ref{ch:optunity}), as most methods feature hyperparameters that must be optimized somehow. Optunity's usefulness is evidenced by its popularity, with hundreds of monthly downloads through the Python Package Index at the time of writing.\footnote{Statistics can be found at \url{https://pypi.python.org/pypi/Optunity}.}

\subsection{Future work} \label{conclusion:ml-future}
Based on the experience gathered and results obtained during our work, we see opportunities for future machine learning research in automated hyperparameter optimization and learning from positive and unlabeled data.

Fully automated machine learning is becoming popular, evidenced by competitions like the ChaLearn AutoML challenge \citep{guyondesign}. A key element of such pipelines is automated hyperparameter optimization, which is receiving a lot of research attention \citep{bergstra2011algorithms, snoek2012practical, bergstra2012random, bergstra2013making, eggensperger2013towards, martinez2014bayesopt, claesen2014easy, eggensperger2015efficient}. Current research focuses on Bayesian optimization based on the somewhat dogmatic belief that these optimizers converge faster than others. We are skeptical towards this claim, as meta-heuristic techniques have been shown competitive in recent publications \citep{papa2015model} as well as our own experiments (see Chapter~\ref{ch:optunity}) and because the famous no free lunch theorem  applies \citep{wolpert1997no}. We believe that meta-heuristic techniques for hyperparameter optimization are currently underdeveloped but promising.

In Chapter~\ref{ch:evaluation} we described how to compute any contingency table-based metric based on only positive and unlabeled data. This is the first approach to compute commonly known metrics, and hence an important contribution, though it does not directly enable computing probablistic performance metrics in general and strictly proper scoring rules like log loss or Brier score in particular. Strictly proper scoring rules \citep{gneiting2007strictly} are especially useful for model selection and calibration as they are more sensitive than metrics like AUROC and AUPR, so extending our approach to enable approximation of these metrics would be useful.


\section{Screening for type 2 diabetes} \label{conclusion:screening}
We set out to investigate the extent to which health expenditure data enables clinical applications like screening for T2D, and thereby improve healthcare. During this research we have successfully developed a proof-of-concept screening method for type 2 diabetes, based exclusively on readily-available health expenditure data collected by the largest Belgian mutual health insurer. Our performance benchmarks have indicated competitiveness to existing screening approaches for T2D that have proven useful in international contexts. Our work can serve as the basis for cost-effective population-wide screening for type 2 diabetes which would strengthen early detection of the disease.

Principally, our screening method identifies patients that will require glucose lowering agents. Given the way we approached the task, we likely have problems identifying patients suffering from mild (pre)diabetes. That said, we treated such patients as negatives in all evaluations, and hence all reported performance estimates are conservative. In fact, our performance estimates are conservative under the reasonable assumption that claims histories of patients with mild diabetes are somewhere between claims histories of healthy patients and those of patients that require GLAs. Interestingly, additional experiments indicate that our approach can identify patients years before GLA therapy is or needs to be initiated, which further adds to the potential of our approach.

Finally, the screening models can also be used to identify drugs and provisions that indicate diabetes risk. This information can then be used to create new medical guidelines or finetune existing ones. Additionally, small subsets of these indictors may be used to create alerts for physicians to help them in screening for diabetes. An example would be to add alerts in the software used by general practitioners when certain combinations of drugs/provisions occur.

\subsection{Weaknesses and limitations of our approach}
The screening approach we developed has a number of weaknesses, related to the labeling of NACM members, limitations of health expenditure data and limitations of machine learning in general.% Each of these are discussed below in more detail.

\subsubsection{Labeling issues}
The quality of learned models hinges upon the quality of the ground truth on which they are based, that is the representativeness of the sets of known positives and (ideally) known negatives for the positive and negative class, respectively. Any labeling bias will perpetuate throughout the modeling process and bias resulting models and performance estimates to some extent.

\paragraph{Known positives have progressed T2D} We identified diabetes patients (positives) based on the routine use of GLA therapy. This labeling omits patients that are exclusively on lifestyle interventions and hence only identifies more severe cases of diabetes. By implication, our models are able to identify patients with profiles that are similar to those on GLA therapy, i.e., patients that likely require GLA therapy, while identifying early stage patients is problematic. 

\paragraph{False positives} Our labeling can flag type 1 diabetes patients, however these patients are usually diagnosed at a young age \citep{american2010diagnosis}. Hence, any bias caused by T1D patients is low when screening in adults above 30. A more severe problem is that we might also flag patients on GLA therapy for alternative reasons (e.g., weight loss \citep{morrison2014metformin}). This is problematic because a significant amount of false positives prohibits reliably estimating performance using the technique outlined in Chapter~\ref{ch:evaluation}. Fortunately, the number of false positives in our setting can reasonably be assumed negligible, though an exact estimate remains unavailable.

\paragraph{No known negatives} As explained in Section~\ref{intro:pulearning}, we had to use semi-supervised learning approaches because it is impossible to directly identify negatives (persons without diabetes). A fully supervised dataset with both positive and negative labels would facilitate learning better models. One way to reliably identify negatives would be to contact individual NACM members to inquire about their health, though this approach may be cost-prohibitive.

\subsubsection{Limitations of health expenditure data}
An important weakness of screening for type 2 diabetes based on health expenditure records is the fact this data source lacks direct indicators for several key risk factors, including lifestyle, diet and genetic predisposition. Luckily, some of this information may nonetheless be available via proxies, e.g., obesity may be subtlely indicated by treatments of hypertension \citep{rahmouni2005obesity}, asthma \citep{sin2008obesity, sutherland2008association}, joint diseases \citep{symmons1997blood, cooper2000risk} and many more health issues.

Additional problems arise when relevant information is omitted from health expenditure databases, e.g., when patients forget to claim refunds. Finally, we must be aware of potentially relevant biases against certain population segments. For example, poverty, lower health literacy and social exclusion are known barriers to accessing healthcare \citep{qualityequality, pleis2009summary, world2010poverty, accesstohealth} while unfortunately the affected population segments are often exposed to elevated health risks \citep{qualityequality, marmot2008closing}. These increased risks include diseases like obesity and diabetes \citep{riste2001high, schillinger2002association}.

\subsubsection{Machine learning limitations}
Machine learning approaches have shown impressive performance on a wide variety of tasks, but it is important to note that machine learning is not a silver bullet that can solve all problems in data analysis. Contemporary machine learning is constrained by limitations which are similar to those imposed on computing in general since the early days of Babbage's Analytical Engine:

\begin{center}
\begin{minipage}{0.93\textwidth}
``\emph{The Analytical Engine has no pretensions whatever to originate anything. It can only do whatever we know how to order it to perform. It can follow analysis, but it has no power of anticipating any analytical revelations or truths. Its province is to assist us in making available what we are already acquainted with.}''
-- Ada Byron, Countess of Lovelace, 1815 -- 1852
\end{minipage}
\end{center}

The notion of generalization performance in machine learning does not defy Lady Lovelace's statement, since generalization performance of learned models is entirely contingent upon the degree to which the assumptions underpinning the learning approach hold. These assumptions are made when designing learning algorithms and object representations. An additional crucial factor is the informativeness of the training data and quality of prior knowledge (if any), as is aptly summarized by the mantra of ``\emph{garbage in, garbage out}''.% both of which are often hard to assess objectively.

The key assumption underlying this entire work is that the health expenditure profiles of undiagnosed diabetes patients are similar in some sense to the health expenditure profiles of patients that started glucose-lowering pharmacotherapy. Furthermore, as we used linear SVM (base) models, the assumed similarity should be captured in the input space representation of persons (cfr. Section~\ref{data}). The performance of our approach indicates that our assumptions are reasonable, though our method is not fit to identify atypical diabetes patients. Such limitations can only be overcome by universal screening.

\subsection{Future work} 

Our research has shown that health expenditure data can effectively be used as a basis for T2D screening programmes with state-of-the-art performance, despite the fact it lacks information on known risk factors for diabetes which are heavily used by other screening approaches (e.g., lifestyle, BMI, genetic predisposition, \ldots). This suggests a lot of potential in screening methods that combine both of these types of information, that is health expenditure data \emph{and} lifestyle, BMI and various clinical parameters, though the potential improvement in predictive performance by linking data sources is difficult to estimate a priori. 

Health expenditure data unifies information across caregivers and provides a fairly complete long-term overview, while other data sources include crucial parameters about lifestyle, genetics and clinical measurements. As such, it is reasonable to assume that these types of data are complementary, and hence their union may allow for screening approaches that far outperform existing approaches. A lot of this information could simply be obtained via patient questionnaires (e.g. BMI, lifestyle, family history, \ldots), though more detailed clinical parameters are harder to procure. Overall, this is a very promising direction for future research, though coupling health expenditure data with other types of information is a sensitive subject from a privacy point of view, so strict adherence to all guidelines and regulations is of paramount importance. 

\subsection{Health expenditure data}
The screening method itself is a proof-of-concept which showcases the potential clinical value of administrative databases such as claims databases maintained by mutual health insurers. The results of the project yield a few conclusions regarding the use of health expenditure data for clinical applications:

\begin{itemize}
\item It is a valuable source of information to build screening programmes for diseases like type 2 diabetes, which prove challenging to detect in contemporary medicine. Its key strengths are that it integrates healthcare information across all caregivers and provides a reliable longitudinal overview of a patient's medical resource-use history. However, resource-use histories are not as detailed as clinical databases maintained by caregivers.
\item It is difficult to find or replicate these strengths elsewhere. Particularly, Belgium does not yet have EHRs that record information from all medical stakeholders into a central, comprehensive database. Implementing such EHRs would be complex for technological, legal, political and psychological reasons. Initiatives like Vitalink\footnote{More info about Vitalink is available at http://www.vitalink.be/.} and the shared pharmaceutical file\footnote{In Dutch: gedeeld farmaceutisch dossier (GFD).} envision parts of this functionality, though these projects are currently in their infancy. For now, claims records remain the sole source of complete, long-term information across medical stakeholders and time.
\end{itemize}

Health expenditure data is already widely used for epidemiology \citep{pladevall2004clinical,lee2006medication,garg2010acute,s23} and, more recently, to assess quality of care \citep{kcequality}. Our work shows that claims data can additionally be used pro-actively to improve healthcare, rather than only in retrospective, descriptive studies. The wealth of information in these databases can likely improve healthcare in many aspects.


\subsection{The elephant in the room}
Our work demonstrated the technological possibility of screening for T2D based on health expenditure data. However, we have not touched upon the ethical, legal and psychological perspectives of data-driven applications in the health and healthcare domain. We conclude this work by briefly discussing some key barriers and open questions concerning applications such as ours.

\subsubsection{Population-wide screening}

The approach outlined in this thesis enables population-wide screening for T2D at a very low operational cost, since we exclusively use readily-available health expenditure data. A major practical question for any population-wide screening approach is how it should be put to use in the real world. 

The first option is to perform risk assessment on the whole population (all members of NACM in our context) and actively contact persons identified to be at high risk. From a medical perspective, such a \emph{push} model seems most appropriate since this has the potential to help a maximal amount of people. 

On the other hand, many people may have qualms about inferences of their health status without the concerned person's explicit prior request and/or approval. A push model as described above can raise issues from a psychological point of view, as it can induce a ``\emph{Big Brother}'' feeling that will surely be resisted by some. Additionally, the \emph{right not to know} has experienced a revived interest from an ethical point of view due to advances in genetic research \citep{ost1984right, andorno2004right, chadwick2014right}, and many of the arguments raised in that discussion essentially also apply to push models which involve proactively notifying patients identified at high risk for some disease.\footnote{The right not to know entails whether patients must be informed regarding some (potentially serious) health problems, even if the patients themselves do not want to know.} A \emph{pull} model, which allows everyone to request the results of a personalized risk analysis at their own leisure, may prove to be more popular. 

\subsubsection{Use of personal data}

Principally, health and healthcare data is considered personal information owned by the involved patient, though often this patient has limited or no access to his or her data and may even be unaware of its existence. This highly sensitive type of personal data is rightfully protected from unpermitted use \citep{privacywet,eu_data}.

Several models exist for patients to permit the usage of personal data for research and various applications after obtaining informed consent. The main distinction is between \emph{opt-in} models, in which data usage is permitted after explicit approval of the patient, and \emph{opt-out} models, in which data can be used unless patients explicitly prohibit it. Opt-in models are most common due to their conservative nature from a privacy point of view, but these may prohibit applications that require a lot of data to create a minimum viable product. 

The use of health and healthcare data is a sensitive matter and policymakers in Belgium and Europe understandably err on the side of conservatism \citep{privacywet, eu_data}, though this inevitably constrains both research and potentially beneficial applications. Recently, the proposal for European data protection regulation raised a lot of criticism, as researchers in health and healthcare worried that an increased amount of conservatism could significantly impede scientific research \citep{feam_2, cumbley2013big, fears2014data, wellcome_position_statement, feam_1}. Our project chimes in on this discussion with a proof-of-concept clinical application based on health expenditure data.

In the end, policymakers must strike a tradeoff between patient privacy vis-\`a-vis potential healthcare improvements through the use of personal data. This sensitive matter is the subject of ongoing debate in both Belgium and Europe. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Keep the following \cleardoublepage at the end of this file, 
% otherwise \includeonly includes empty pages.
\cleardoublepage

% vim: tw=70 nocindent expandtab foldmethod=marker foldmarker={{{}{,}{}}}
